{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tfidf_test.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3hIcgg8eIrgk","executionInfo":{"status":"ok","timestamp":1616045827310,"user_tz":-330,"elapsed":13915,"user":{"displayName":"Keshav Kabra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoCztJqz_VMpgFabxO-2-1m05tUq3lBATXSVqRjQ=s64","userId":"18144280787432046006"}},"outputId":"e7556a8a-3769-4485-cb50-8c6491f442ec"},"source":["import pandas as pd\n","import nltk\n","import re\n","from bs4 import BeautifulSoup\n","import numpy as np\n","import torch\n","!pip install transformers\n","import transformers as ppb \n","nltk.download('stopwords')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/d8/5144b0712f7f82229a8da5983a8fbb8d30cec5fbd5f8d12ffe1854dcea67/transformers-4.4.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 4.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 17.2MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 46.0MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=105ad20cc530df087528a584bf88e64f118079143505adeff87c61865b17b1ba\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.1\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"z51QTzj4JFKa"},"source":["REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","STOPWORDS =nltk.corpus.stopwords.words('english')\n","\n","def clean_text(text):\n","    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n","    text = text.lower() # lowercase text\n","    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n","    return text\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVB4e_q1JIwP","outputId":"0da50df6-7a0d-4c51-f43d-6795b48afb22"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf = TfidfVectorizer()\n","fn=['Train','Test']\n","for k in range(0,1):\n","    fname='/content/'+fn[k]+'.csv'\n","    df = pd.read_csv(fname,encoding='latin-1')\n","    df['tweet'] = df['tweet'].apply(clean_text)\n","    print(df.head(10))\n","    x = tfidf.fit_transform(df['tweet'])\n","    df_tfidf = pd.DataFrame(x.toarray())\n","    x=np.array(df_tfidf)\n","    fname='/content/tfidf_'+fn[k]+'.csv'\n","    np.savetxt(fname,x, delimiter=',', fmt='%f')  \n"," \n","\n","\n","\n","noc=np.zeros((x.shape[1]))\n","for i in range(0,x.shape[1]):\n","    y=x[:,i]\n","    in1=np.where(y!=0)\n","    noc[i]=len(in1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["   id  label                                              tweet\n","0   1      0  user father dysfunctional selfish drags kids d...\n","1   2      0  user user thanks #lyft credit cant use cause d...\n","2   3      0                                     bihday majesty\n","3   4      0                       #model love u take u time ur\n","4   5      0                     factsguide society #motivation\n","5   6      0  2 2 huge fan fare big talking leave chaos pay ...\n","6   7      0  user camping tomorrow user user user user user...\n","7   8      0  next school year year exams cant think #school...\n","8   9      0  love land #allin #cavs #champions #cleveland #...\n","9  10      0                          user user welcome im #gr8\n"],"name":"stdout"}]}]}