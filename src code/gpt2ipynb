{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gpt2ipynb","provenance":[],"authorship_tag":"ABX9TyMR88t73FtNVJu1hAm9n0qU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muyJ70-hryAY","executionInfo":{"status":"ok","timestamp":1616043789344,"user_tz":-330,"elapsed":10466,"user":{"displayName":"Keshav Kabra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgoCztJqz_VMpgFabxO-2-1m05tUq3lBATXSVqRjQ=s64","userId":"18144280787432046006"}},"outputId":"6d22ba85-f5e5-4614-cfe9-90d0286649dd"},"source":["!pip install transformers\n","from transformers import OpenAIGPTTokenizer, OpenAIGPTModel\n","import torch\n","from transformers import GPT2LMHeadModel\n","from transformers import GPT2Tokenizer\n","import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","import re\n","from bs4 import BeautifulSoup\n","import numpy as np\n","import torch\n","import transformers as ppb "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IZqf6Zf1sDgi"},"source":["REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","STOPWORDS =nltk.corpus.stopwords.words('english')\n","\n","def clean_text(text):\n","    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n","    text = text.lower() # lowercase text\n","    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n","    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n","    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n","    return text\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLMfsmZOsQko"},"source":["model = GPT2LMHeadModel.from_pretrained('gpt2')\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","\n","def gpt_fscore(text,model,tokenizer):\n","    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0) \n","    outputs = model(input_ids)\n","    last_hidden_states = outputs[0]\n","    return last_hidden_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYlsZO5DsglR","outputId":"fdab9073-1698-4049-c21a-60a6b6ac97e0"},"source":["fn=['train']\n","for k in range(0,1):\n","    fname='/content/'+fn[k]+'.csv'\n","    df = pd.read_csv(fname,encoding='latin-1')\n","    df['tweet'] = df['tweet'].apply(clean_text)\n","    print(df.head(10))\n","    train_tokens = list(map(lambda t:   tokenizer.tokenize(t)+['[CLS]'], df['tweet']))\n","    train_tokens_ids = list(map(tokenizer.convert_tokens_to_ids,train_tokens))\n","    b=np.zeros((np.shape(df)[0]))\n","    for i in range(0,np.shape(df)[0]):\n","        b[i]=np.shape(train_tokens_ids[i])[0]\n","    pad=np.zeros((np.shape(df)[0],int(np.max(b))))\n","    for i in range(0,np.shape(df)[0]):\n","        a=train_tokens_ids[i]\n","        for j in range(0,np.shape(a)[0]):\n","            pad[i,j]=a[j]\n","    \n","    pad1=pad[:,:]       \n","    input_ids = torch.tensor(np.array(pad1))        \n","    with torch.no_grad():\n","        last_hidden_states = model(input_ids.long())    \n","    features = last_hidden_states[0][:,0,:].numpy()\n","    fname='/content/gpt2_'+fn[k]+'.csv'\n","    np.savetxt(fname,features, delimiter=',', fmt='%f') "],"execution_count":null,"outputs":[{"output_type":"stream","text":["   id  label                                              tweet\n","0   1      0  user father dysfunctional selfish drags kids d...\n","1   2      0  user user thanks #lyft credit cant use cause d...\n","2   3      0                                     bihday majesty\n","3   4      0                       #model love u take u time ur\n","4   5      0                     factsguide society #motivation\n","5   6      0  2 2 huge fan fare big talking leave chaos pay ...\n","6   7      0  user camping tomorrow user user user user user...\n","7   8      0  next school year year exams cant think #school...\n","8   9      0  love land #allin #cavs #champions #cleveland #...\n","9  10      0                          user user welcome im #gr8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LlVJXacKuvea"},"source":[""],"execution_count":null,"outputs":[]}]}